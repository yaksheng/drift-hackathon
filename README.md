# Drift Hackathon - GalaxyRVR Autonomous Navigation

## ðŸŽ¯ Project Overview

This project implements an autonomous navigation system for the GalaxyRVR robot using computer vision and sensor fusion. The system combines overhead camera vision for global path planning with onboard sensors for local obstacle avoidance.

## ðŸ¤– System Architecture

### Hardware Components

- **GalaxyRVR Robot**
  - Differential drive motors (left/right)
  - ESP32-CAM for onboard vision
  - Ultrasonic distance sensor (forward)
  - IR obstacle sensors (left/right)
  - Servo-controlled camera pan
  - Battery voltage monitoring

- **Overhead Vision System**
  - USB webcam positioned above arena
  - Real-time perspective transformation
  - Red corner marker detection for calibration

### Software Components

```
drift-hackathon/
â”œâ”€â”€ robot_code/
â”‚   â”œâ”€â”€ arduino/              # Robot firmware
â”‚   â””â”€â”€ python_client/        # Robot control library
â”œâ”€â”€ webcam_code/              # Overhead camera system
â”‚   â”œâ”€â”€ arena_transform.py   # Perspective transformation
â”‚   â”œâ”€â”€ webcam_stream.py     # Camera streaming server
â”‚   â””â”€â”€ webcam_client.py     # Camera client
â””â”€â”€ autonomous_navigation/    # Main navigation system (to be implemented)
```

## ðŸ“‹ Challenge Analysis

Based on the codebase analysis, the challenge likely involves:

1. **Autonomous Navigation**: Robot must navigate to targets/waypoints in an arena
2. **Computer Vision**: Detect and track objects/targets using vision
3. **Obstacle Avoidance**: Navigate around obstacles using sensors
4. **Precision Control**: Accurate positioning and path following

## ðŸš€ Implementation Plan

### Phase 1: System Integration & Calibration âœ…

- [x] Repository setup and initial code commit
- [ ] Arena setup and red corner marker verification
- [ ] World coordinate system calibration
- [ ] Perspective transformation accuracy testing
- [ ] Robot connection and sensor validation

### Phase 2: Core Navigation System ðŸš§

#### 2.1 Target Detection Module
- [ ] Implement color-based target detection (HSV)
- [ ] Shape detection using contours
- [ ] Multiple target tracking
- [ ] Coordinate transformation (pixel â†’ world coordinates)

#### 2.2 Robot Localization
- [ ] Robot position tracking in overhead view
- [ ] Marker-based or tracking-based localization
- [ ] World coordinate mapping
- [ ] Position update loop

#### 2.3 Path Planning
- [ ] Simple straight-line navigation
- [ ] Obstacle-aware path planning
- [ ] Waypoint following
- [ ] Dynamic path adjustment

#### 2.4 Control System
- [ ] PID controller for heading
- [ ] Speed control based on distance
- [ ] Sensor fusion (ultrasonic + IR + vision)
- [ ] State machine for navigation modes

### Phase 3: Advanced Features ðŸŽ¯

- [ ] Hybrid vision system (overhead + onboard)
- [ ] Obstacle avoidance integration
- [ ] Error recovery mechanisms
- [ ] Performance optimization
- [ ] Real-time visualization

### Phase 4: Testing & Refinement ðŸ§ª

- [ ] Unit testing for modules
- [ ] Integration testing
- [ ] Arena testing and calibration
- [ ] Performance tuning
- [ ] Documentation

## ðŸ—ï¸ Technical Approach

### Strategy: Hybrid Vision System (Recommended)

**Primary: Overhead Vision**
- Global view of entire arena
- Easier target detection
- Better path planning
- Accurate robot positioning

**Secondary: Onboard Vision**
- Local navigation refinement
- Target verification
- Obstacle detection

**Tertiary: Sensor Fusion**
- Ultrasonic for forward obstacles
- IR sensors for side obstacles
- Vision for target tracking

### Navigation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Overhead Cameraâ”‚
â”‚  (Global View)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Target Detectionâ”‚
â”‚ & Localization  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Path Planning â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Robot Camera â”‚
         â”‚          â”‚ (Local View) â”‚
         â–¼          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  Robot Control  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  (Motor Commands)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sensor Feedback â”‚
â”‚ (Ultrasonic/IR) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Adjust Path     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### State Machine

```
IDLE
  â”‚
  â–¼
SEARCHING (detect targets)
  â”‚
  â–¼
TRACKING (lock onto target)
  â”‚
  â–¼
NAVIGATING (move toward target)
  â”‚
  â”œâ”€â–º OBSTACLE_DETECTED â”€â”€â–º AVOIDING â”€â”€â–º NAVIGATING
  â”‚
  â–¼
ARRIVED (at target)
  â”‚
  â–¼
RETURNING (optional: return to start)
```

## ðŸ› ï¸ Setup Instructions

### Prerequisites

```bash
# Python dependencies
pip install -r robot_code/python_client/requirements.txt
pip install -r webcam_code/requirements.txt

# Additional dependencies for navigation
pip install scipy matplotlib
```

### Robot Configuration

1. **Arduino Setup**
   - Upload `galaxy-rvr.ino` to Arduino
   - Configure WiFi credentials in the code
   - Verify WebSocket connection on port 8765

2. **Network Configuration**
   - Ensure robot and computer are on same network
   - Note robot IP address (default: 192.168.1.216)
   - Test connection with `simple_control.py`

### Arena Setup

1. **Physical Setup**
   - Place red corner markers at arena corners
   - Position overhead webcam above arena
   - Ensure good lighting conditions

2. **Calibration**
   ```bash
   # Run arena transformation calibration
   cd webcam_code
   python arena_transform.py
   ```

3. **Start Camera Stream**
   ```bash
   # Start webcam server with transformation
   python webcam_stream.py --transform --camera 0
   ```

## ðŸ“ Project Structure

```
drift-hackathon/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ robot_code/
â”‚   â”œâ”€â”€ arduino/
â”‚   â”‚   â””â”€â”€ galaxy-rvr/                # Arduino firmware
â”‚   â””â”€â”€ python_client/
â”‚       â”œâ”€â”€ galaxyrvr.py              # Robot control library
â”‚       â”œâ”€â”€ galaxyrvr_camera.py       # Camera stream helper
â”‚       â”œâ”€â”€ galaxyrvr_keyboard.py     # Keyboard control
â”‚       â”œâ”€â”€ simple_control.py         # Simple control example
â”‚       â””â”€â”€ requirements.txt
â”œâ”€â”€ webcam_code/
â”‚   â”œâ”€â”€ arena_transform.py            # Perspective transformation
â”‚   â”œâ”€â”€ webcam_stream.py              # Camera streaming server
â”‚   â”œâ”€â”€ webcam_client.py              # Camera client
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ autonomous_navigation/            # Main navigation system (TODO)
    â”œâ”€â”€ target_detection.py           # Target detection module
    â”œâ”€â”€ robot_localization.py         # Position tracking
    â”œâ”€â”€ path_planner.py               # Path planning
    â”œâ”€â”€ navigation_controller.py      # Control system
    â””â”€â”€ main.py                       # Main entry point
```

## ðŸ”§ Key Modules to Implement

### 1. Target Detection (`target_detection.py`)
- Color-based detection using HSV
- Contour analysis for shape detection
- Multiple target tracking
- Coordinate conversion utilities

### 2. Robot Localization (`robot_localization.py`)
- Marker-based position detection
- Tracking-based localization
- World coordinate transformation
- Position filtering and smoothing

### 3. Path Planner (`path_planner.py`)
- Straight-line path calculation
- Obstacle-aware path planning
- Waypoint generation
- Dynamic path adjustment

### 4. Navigation Controller (`navigation_controller.py`)
- PID control for heading
- Speed control
- Sensor fusion
- State machine management

### 5. Main Navigation Loop (`main.py`)
- Integration of all modules
- Real-time control loop
- Error handling and recovery
- Visualization and logging

## ðŸŽ® Usage

### Basic Robot Control
```bash
cd robot_code/python_client
python simple_control.py
```

### Camera Streaming
```bash
cd webcam_code
python webcam_stream.py --transform
```

### Autonomous Navigation (Once Implemented)
```bash
cd autonomous_navigation
python main.py --robot-ip 192.168.1.216 --camera-url http://localhost:8000/
```

## ðŸ“Š Performance Metrics

- **Target Detection Accuracy**: >95%
- **Position Accuracy**: <5cm error
- **Navigation Success Rate**: >90%
- **Obstacle Avoidance**: 100% (no collisions)
- **Response Time**: <100ms control loop

## ðŸ› Troubleshooting

### Robot Connection Issues
- Verify WiFi credentials in Arduino code
- Check robot IP address
- Ensure WebSocket port 8765 is accessible

### Camera Issues
- Verify camera index (try 0, 1, 2...)
- Check red corner marker visibility
- Adjust HSV thresholds for lighting

### Navigation Issues
- Recalibrate arena transformation
- Verify world coordinates
- Check sensor readings

## ðŸ“ Notes

- The arena uses red corner markers for calibration
- World coordinates are defined in `arena_transform.py`
- Robot communicates via WebSocket on port 8765
- Camera streams on port 9000 (robot) and 8000 (webcam)

## ðŸ¤ Contributing

This is a hackathon project. Key areas for contribution:
- Target detection algorithms
- Path planning improvements
- Sensor fusion techniques
- Performance optimization

## ðŸ“„ License

Initial hackathon code provided by Drift.

---

**Status**: ðŸš§ In Development
**Last Updated**: Initial commit
**Next Steps**: Implement Phase 2 modules

